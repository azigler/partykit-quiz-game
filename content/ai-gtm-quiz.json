{
  "metadata": {
    "title": "AI Measurement Framework Quiz",
    "description": "Test your knowledge of measuring AI effectiveness in engineering teams using LinearB's proven framework",
    "version": "3.0.0",
    "created": "2025-09-15",
    "totalQuestions": 15
  },
  "questions": [
    {
      "id": "lb-001",
      "question": "According to LinearB's philosophy, what are the 'true signals' for measuring AI effectiveness?",
      "options": [
        "Lines of code generated and time saved",
        "Throughput and quality",
        "Developer satisfaction and tool adoption rates",
        "Cost reduction and velocity improvements"
      ],
      "correctAnswer": 1,
      "category": "Philosophy",
      "difficulty": "easy",
      "points": 5,
      "explanation": "LinearB's core philosophy: 'Throughput & Quality are the true signals. Not lines of code. Not time saved. What matters is: are you shipping more value, with fewer mistakes?'"
    },
    {
      "id": "lb-002", 
      "question": "LinearB defines AI ROI as which of the following?",
      "options": [
        "Time saved in hours multiplied by developer salary costs",
        "Features shipped, bugs avoided, and developer energy focused on high-leverage work",
        "Lines of code generated per developer per sprint",
        "Reduced hiring needs due to increased productivity"
      ],
      "correctAnswer": 1,
      "category": "ROI Definition",
      "difficulty": "easy", 
      "points": 5,
      "explanation": "LinearB emphasizes 'Track value delivered, not hours eliminated. AI ROI = features shipped, bugs avoided, and developer energy focused on high-leverage work.'"
    },
    {
      "id": "lb-003",
      "question": "How does LinearB recommend thinking about AI adoption in your organization?",
      "options": [
        "Count headcount of developers using AI tools",
        "Focus on mindsets across a spectrum from skeptical to embracing",
        "Measure percentage of code commits using AI",
        "Track monthly active users of AI tools"
      ],
      "correctAnswer": 1,
      "category": "Adoption Strategy",
      "difficulty": "medium",
      "points": 10,
      "explanation": "LinearB's philosophy: 'Think in mindsets, not headcounts. Developers fall on a spectrum between skeptical and embracing. Find both ends within your organization and understand their viewpoints.'"
    },
    {
      "id": "lb-004",
      "question": "What does LinearB identify as the 'starting line' versus 'finish line' of AI adoption maturity?",
      "options": [
        "Junior developers vs Senior developers using AI",
        "Low context, iterating on outputs vs High context, iterating on inputs",
        "Individual adoption vs Team-wide adoption",
        "Basic code completion vs Advanced code generation"
      ],
      "correctAnswer": 1,
      "category": "Maturity Model",
      "difficulty": "hard",
      "points": 15,
      "explanation": "LinearB describes the journey as: Starting line = low context, iterating on outputs, delayed technical debt. Finish line = high context, iterating on inputs, preventing technical debt."
    },
    {
      "id": "lb-005",
      "question": "Which metric does LinearB use to measure how refined a pull request is when initially submitted?",
      "options": [
        "Code coverage percentage",
        "PR Maturity",
        "Review turnaround time",
        "Lines of code changed"
      ],
      "correctAnswer": 1,
      "category": "Quality Metrics",
      "difficulty": "medium",
      "points": 10,
      "explanation": "PR Maturity measures how well-prepared a PR is when submitted for review, assessing changes needed post-submission to address feedback and improve the PR."
    },
    {
      "id": "lb-006",
      "question": "LinearB's 'Rework Rate' metric specifically measures what?",
      "options": [
        "How often developers reject AI suggestions",
        "Changes to code modified within the last 21 days",
        "Time spent refactoring legacy code",
        "Percentage of PRs requiring multiple review cycles"
      ],
      "correctAnswer": 1,
      "category": "Quality Metrics", 
      "difficulty": "hard",
      "points": 15,
      "explanation": "Rework Rate measures changes to recent code (modified within last 21 days). High rework may indicate problems with initial implementation or unclear requirements."
    },
    {
      "id": "lb-007",
      "question": "What is LinearB's unique differentiator for tracking AI-generated code?",
      "options": [
        "Integration with all major AI coding tools",
        "Using gitStream to label AI-generated PRs at the source",
        "Real-time monitoring of developer productivity",
        "Automated surveys sent to developers"
      ],
      "correctAnswer": 1,
      "category": "Technical Approach",
      "difficulty": "medium",
      "points": 10,
      "explanation": "Only LinearB uses gitStream to label AI-generated PRs at the source, enabling tool-to-tool comparison and ongoing quality tracking."
    },
    {
      "id": "lb-008",
      "question": "According to LinearB's framework, why is 'throughput alone â‰  value'?",
      "options": [
        "Because faster development often leads to more bugs",
        "Because business impact isn't always correlated with code volume", 
        "Because developers need work-life balance",
        "Because leadership prefers predictable delivery schedules"
      ],
      "correctAnswer": 1,
      "category": "Philosophy",
      "difficulty": "medium",
      "points": 10,
      "explanation": "LinearB uses Ori's example: spending 2 months hunting a memory leak with a 1-line fix had immense business value, showing that lines of code never truly maps to business impact."
    },
    {
      "id": "lb-009",
      "question": "What does LinearB recommend as the best methodology for measuring AI impact?",
      "options": [
        "Before-and-after comparisons of team velocity",
        "Using gitStream to label and compare AI vs non-AI PRs",
        "Tracking story points completed per developer",
        "Measuring daily active users of AI tools"
      ],
      "correctAnswer": 1,
      "category": "Methodology",
      "difficulty": "hard",
      "points": 15,
      "explanation": "LinearB recommends using gitStream to label & compare PRs between groups (AI vs. not) with appropriate sample size to draw meaningful conclusions."
    },
    {
      "id": "lb-010",
      "question": "LinearB's framework combines quantitative data with qualitative insights. What's a key qualitative method?",
      "options": [
        "Code quality automated analysis",
        "Developer surveys sent through Slack/Teams",
        "Automated PR review comments",
        "API data from AI tool providers"
      ],
      "correctAnswer": 1,
      "category": "Data Collection",
      "difficulty": "easy",
      "points": 5,
      "explanation": "LinearB emphasizes developer surveys sent through Slack/MS Teams to capture sentiment: 'You need to hear from developers if the tool is perceived as valuable to them.'"
    },
    {
      "id": "lb-011",
      "question": "When presenting AI effectiveness to leadership, LinearB recommends focusing on:",
      "options": [
        "Time saved metrics and cost reduction spreadsheets",
        "Product momentum through features and quality",
        "Developer satisfaction scores and retention rates", 
        "Technical metrics like code coverage and complexity"
      ],
      "correctAnswer": 1,
      "category": "Leadership Communication",
      "difficulty": "medium",
      "points": 10,
      "explanation": "LinearB states: 'Leadership wants to see product momentum, not spreadsheets. Currency of Measurement = Features, Value, Quality. Not time saved or dollars saved.'"
    },
    {
      "id": "lb-012",
      "question": "What does LinearB mean by 'encouraging better context engineering'?",
      "options": [
        "Training developers to write better prompts and inputs",
        "Improving code documentation and comments", 
        "Setting up better development environments",
        "Creating more detailed user stories and requirements"
      ],
      "correctAnswer": 0,
      "category": "Best Practices",
      "difficulty": "hard",
      "points": 15,
      "explanation": "LinearB emphasizes that 'Context matters. Inputs shape outcomes. Encouraging better context engineering is as important as measuring output volume' - referring to helping developers craft better AI prompts."
    },
    {
      "id": "lb-013",
      "question": "According to LinearB, what characterizes the difference between junior and senior developer AI usage?",
      "options": [
        "Seniors use more advanced AI tools than juniors",
        "Juniors iterate on outputs, seniors iterate on inputs", 
        "Seniors are more skeptical of AI assistance",
        "Juniors generate more lines of code with AI"
      ],
      "correctAnswer": 1,
      "category": "Developer Maturity",
      "difficulty": "hard", 
      "points": 15,
      "explanation": "LinearB notes: 'Junior: run with it, put out fires, iterate on outputs. Sr: monitor performance, keep it out of bad loops, re-eval prompts, iterate inputs.'"
    },
    {
      "id": "lb-014",
      "question": "LinearB's framework tracks 'Time to Merge (TTM)' to measure:",
      "options": [
        "How fast developers can write code with AI",
        "Whether AI is improving areas it's supposed to improve",
        "Developer productivity compared to manual coding",
        "The speed of code review processes"
      ],
      "correctAnswer": 1,
      "category": "Impact Metrics",
      "difficulty": "medium",
      "points": 10,
      "explanation": "TTM tracks 'total time it takes for an approved pull request to be merged into the main branch' to answer: 'Are the areas in which GenAI is subject to improve, actually improving?'"
    },
    {
      "id": "lb-015",
      "question": "Why does LinearB emphasize that AI adoption is 'a marathon, not a race'?",
      "options": [
        "Because AI tools are still evolving and improving",
        "Because it takes time to see ROI from AI investments",
        "Because developer mindset change requires patience and understanding",
        "Because implementing AI tools across large teams is complex"
      ],
      "correctAnswer": 2,
      "category": "Change Management",
      "difficulty": "medium",
      "points": 10,
      "explanation": "LinearB's philosophy emphasizes understanding developer mindsets across the skeptical-to-embracing spectrum, requiring patience to help teams mature from low-context output iteration to high-context input iteration."
    }
  ]
}