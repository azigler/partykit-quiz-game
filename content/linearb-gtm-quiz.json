{
  "title": "LinearB GTM AI Quiz",
  "description": "Blend of LinearB\u2019s AI Measurement Framework and industry fundamentals to educate externals and test GTM mastery.",
  "version": "1.1.0",
  "created": "2025-09-16",
  "totalQuestions": 10,
  "questions": [
    {
      "id": "lb-gtm-001",
      "question": "What does LinearB identify as the 'true signals' for measuring AI effectiveness?",
      "options": [
        "Lines of code written and time saved",
        "Throughput and quality",
        "Number of AI tools adopted per developer",
        "Cost savings from reduced headcount"
      ],
      "correctAnswer": 1,
      "category": "Philosophy",
      "difficulty": "easy",
      "points": 5,
      "explanation": "LinearB emphasizes that throughput & quality are the true signals \u2014 not lines of code or hours saved\u301017\u2020source\u3011.",
      "tags": [
        "framework",
        "signals",
        "quality",
        "throughput"
      ]
    },
    {
      "id": "lb-gtm-002",
      "question": "How does the AI adoption journey progress according to LinearB\u2019s framework?",
      "options": [
        "From junior developers to senior developers",
        "From low-context, iterating on outputs to high-context, iterating on inputs",
        "From fewer to more AI tools used",
        "From reduced cost to increased velocity"
      ],
      "correctAnswer": 1,
      "category": "Maturity Model",
      "difficulty": "medium",
      "points": 10,
      "explanation": "The framework describes a journey: starting with low context and output iteration, and maturing into high context, input iteration to prevent technical debt\u301017\u2020source\u3011.",
      "tags": [
        "adoption",
        "maturity",
        "context engineering"
      ]
    },
    {
      "id": "lb-gtm-003",
      "question": "LinearB defines AI ROI in terms of:",
      "options": [
        "Time saved multiplied by developer salaries",
        "Features shipped, bugs avoided, and energy focused on high-leverage work",
        "Headcount reduction from automation",
        "Percentage of commits generated by AI"
      ],
      "correctAnswer": 1,
      "category": "ROI",
      "difficulty": "easy",
      "points": 5,
      "explanation": "LinearB\u2019s ROI = value delivered: features shipped, bugs avoided, and energy applied to higher-leverage tasks\u301017\u2020source\u3011.",
      "tags": [
        "roi",
        "value",
        "leadership"
      ]
    },
    {
      "id": "lb-gtm-004",
      "question": "What is the purpose of using gitStream to label AI-generated PRs?",
      "options": [
        "To identify junior vs senior developer code",
        "To enable tool-to-tool comparisons and track quality across AI vs non-AI PRs",
        "To enforce compliance with open source licenses",
        "To automatically reject low-quality PRs"
      ],
      "correctAnswer": 1,
      "category": "Differentiator",
      "difficulty": "medium",
      "points": 10,
      "explanation": "gitStream labeling is LinearB\u2019s differentiator: it allows comparison between AI and non-AI PRs and supports continuous impact tracking\u301017\u2020source\u3011.",
      "tags": [
        "gitStream",
        "labeling",
        "evaluation"
      ]
    },
    {
      "id": "lb-gtm-005",
      "question": "Which quality metric measures how refined a pull request is when submitted?",
      "options": [
        "Review cycle count",
        "PR Maturity",
        "Lines of code changed",
        "Defect density"
      ],
      "correctAnswer": 1,
      "category": "Quality Metrics",
      "difficulty": "medium",
      "points": 10,
      "explanation": "PR Maturity indicates how ready a PR is at first submission, showing if AI is helping developers submit cleaner code\u301017\u2020source\u3011.",
      "tags": [
        "pr maturity",
        "quality"
      ]
    },
    {
      "id": "lb-gtm-006",
      "question": "What does Rework Rate measure in the LinearB framework?",
      "options": [
        "Number of AI suggestions rejected",
        "Changes to code modified within the last 21 days",
        "Hours spent refactoring legacy code",
        "Percentage of PRs requiring multiple approvals"
      ],
      "correctAnswer": 1,
      "category": "Quality Metrics",
      "difficulty": "hard",
      "points": 15,
      "explanation": "Rework Rate measures changes to recently modified code; high rework indicates quality or planning gaps\u301017\u2020source\u3011.",
      "tags": [
        "rework",
        "quality"
      ]
    },
    {
      "id": "lb-gtm-007",
      "question": "Even with very large context windows, why is retrieval (RAG) still useful in engineering workflows?",
      "options": [
        "Because large windows increase model latency",
        "Because retrieval provides fresh, scoped, and verifiable context that may exceed the window and reduces hallucinations",
        "Because retrieval makes prompts shorter",
        "Because retrieval automatically fixes security bugs"
      ],
      "correctAnswer": 1,
      "category": "AI Foundations",
      "difficulty": "medium",
      "points": 10,
      "explanation": "Retrieval helps ground the model in current, relevant artifacts (code, tickets, docs) when corpora are too big or need factual grounding.",
      "tags": [
        "RAG",
        "context window",
        "grounding"
      ]
    },
    {
      "id": "lb-gtm-008",
      "question": "Which statement best describes an agentic AI workflow in developer tools?",
      "options": [
        "A single prompt that outputs code from scratch",
        "A system that plans, calls tools/APIs, observes results, and iterates toward a goal",
        "A chatbot that answers questions without external tools",
        "A code generator that only suggests completions in the IDE"
      ],
      "correctAnswer": 1,
      "category": "AI Foundations",
      "difficulty": "easy",
      "points": 5,
      "explanation": "Agentic systems plan-act-observe-iterate and use tools to accomplish tasks, which is distinct from single-turn prompting.",
      "tags": [
        "agents",
        "tools",
        "iteration"
      ]
    },
    {
      "id": "lb-gtm-009",
      "question": "Which pair is the most reliable starting point for measuring AI\u2019s impact on delivery outcomes across teams?",
      "options": [
        "Lines of code and hours saved",
        "Lead time for changes and change failure rate (augmented with an internal quality metric)",
        "Daily active users of AI tools and prompts per day",
        "Number of AI tools installed and accepted suggestions"
      ],
      "correctAnswer": 1,
      "category": "Measurement",
      "difficulty": "medium",
      "points": 10,
      "explanation": "Outcomes > output: delivery speed and quality (e.g., lead time and failure rates) reveal impact better than volume-based proxies.",
      "tags": [
        "DORA",
        "delivery",
        "quality"
      ]
    },
    {
      "id": "lb-gtm-010",
      "question": "To fairly compare two AI coding tools, which evaluation setup is best?",
      "options": [
        "Switch tools mid-sprint and ask developers how it felt",
        "Label AI vs non-AI PRs and run matched cohort comparisons over a statistically meaningful sample",
        "Compare lines of code generated per day for each tool",
        "Use only vendor-reported dashboards"
      ],
      "correctAnswer": 1,
      "category": "Evaluation",
      "difficulty": "hard",
      "points": 15,
      "explanation": "Cohort-based comparisons with consistent labeling (e.g., via gitStream) and adequate sample sizes provide defensible evidence\u301017\u2020source\u3011.",
      "tags": [
        "evaluation",
        "cohorts",
        "gitStream"
      ]
    }
  ]
}